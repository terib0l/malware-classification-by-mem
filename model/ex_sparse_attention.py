import tensorflow as tf
from model.basic import BasicModel

class SupervisedUMAP(BasicModel):
    def binary_case_basic_flow(self):
        base_model = self.choose_convolutional_model()

        model = tf.keras.models.Sequential()
        model.add(base_model)
        model.add(tf.keras.layers.Flatten())
        model.add(tf.keras.layers.Dense(64, activation='relu'))
        model.add(tf.keras.layers.Dense(1, activation='sigmoid'))
        model.summary()

        model.compile(
            optimizer='adam',
            loss='binary_crossentropy',
            metrics=[
                tf.keras.metrics.BinaryAccuracy(name='accuracy'),
                tf.keras.metrics.Precision(name='precision'),
                tf.keras.metrics.Recall(name='recall'),
            ]
        )

        model.fit(
            x=self.x_train,
            y=self.y_train,
            batch_size=32,
            epochs=50,
            callbacks=[
                tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)
            ],
            verbose='auto'
        )

        return model


    def category_case_basic_flow(self):
        base_model = self.choose_convolutional_model()

        model = tf.keras.models.Sequential()
        model.add(base_model)
        model.add(tf.keras.layers.Flatten())
        model.add(tf.keras.layers.Dense(64, activation='relu'))
        model.add(tf.keras.layers.Dense(11, activation='softmax'))
        model.summary()

        model.compile(
            optimizer='adam',
            loss='sparse_categorical_crossentropy',
            metrics=['accuracy']
        )

        model.fit(
            x=self.x_train,
            y=self.y_train,
            batch_size=32,
            epochs=50,
            callbacks=[
                tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)
            ],
            verbose='auto'
        )
        y_pred = model.predict(self.x_test).argmax(axis=1)

        return y_pred


import tensorflow as tf
from tensorflow.keras import models, layers
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.applications import Xception
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout, Flatten, Input, Conv2D, multiply, LocallyConnected2D, Lambda, BatchNormalization
from tensorflow.keras.models import Model
from tensorflow.keras.metrics import mean_absolute_error

class Ref:
    def __init__():
        pass

    def create_model():
        input_shape = (TARGET_SIZE, TARGET_SIZE, 3)
        in_lay = Input(input_shape)
        conv_base = Xception(include_top = False, weights = 'imagenet', input_shape = input_shape)
        pt_features = conv_base(in_lay)
        bn_features = BatchNormalization()(pt_features)

        # here we do an attention mechanism to turn pixels in the GAP on an off
        attn_layer = Conv2D(64, kernel_size = (1,1), padding = 'same', activation = 'relu')(bn_features)
        attn_layer = Conv2D(16, kernel_size = (1,1), padding = 'same', activation = 'relu')(attn_layer)
        attn_layer = LocallyConnected2D(1, kernel_size = (1,1), padding = 'valid', activation = 'sigmoid')(attn_layer)
        # fan it out to all of the channels
        pt_depth = conv_base.get_output_shape_at(0)[-1]
        up_c2_w = np.ones((1, 1, 1, pt_depth))
        up_c2 = Conv2D(pt_depth, kernel_size = (1,1), padding = 'same', 
                    activation = 'linear', use_bias = False, weights = [up_c2_w])
        up_c2.trainable = False
        attn_layer = up_c2(attn_layer)

        mask_features = multiply([attn_layer, bn_features])
        gap_features = GlobalAveragePooling2D()(mask_features)
        gap_mask = GlobalAveragePooling2D()(attn_layer)
        # to account for missing values from the attention model
        gap = Lambda(lambda x: x[0]/x[1], name = 'RescaleGAP')([gap_features, gap_mask])
        gap_dr = Dropout(0.5)(gap)
        dr_steps = Dropout(0.25)(Dense(1024, activation = 'elu')(gap_dr))
        out_layer = Dense(11, activation = 'sigmoid')(dr_steps)
        model = Model(inputs = [in_lay], outputs = [out_layer])
        model.compile(optimizer = Adam(lr = 0.002), loss = 'binary_crossentropy', metrics = ["AUC"])
        return model

    def run():
        with tpu_strategy.scope():
            model = create_model()
        model.summary()

        history = model.fit(
            train_df,
            epochs = EPOCHS,
            steps_per_epoch = STEPS_PER_EPOCH,
            validation_data = valid_df,
            validation_steps = VALIDATION_STEPS
        )
