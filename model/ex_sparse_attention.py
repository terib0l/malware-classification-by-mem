import numpy as np
import tensorflow as tf
from tensorflow.keras import models, layers
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.applications import Xception
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout, Flatten, Input, Conv2D, multiply, LocallyConnected2D, Lambda, BatchNormalization
from tensorflow.keras.models import Model
from tensorflow.keras.metrics import mean_absolute_error
from model.basic import BasicModel

# link: https://stackoverflow.com/questions/65598647/dynamic-spatial-convolution-is-not-supported-on-tpu

class DynamicSpatialAttention(BasicModel):
    def binary_case_basic_flow(self):
        input_shape = (256, 256, 3)
        in_lay = Input(input_shape)
        # conv_base = Xception(
        #     include_top = False,
        #     weights = 'imagenet',
        #     input_shape = input_shape
        # )
        conv_base = self.choose_convolutional_model()

        """
        Dynamic Spatial Attention
        """
        pt_features = conv_base(in_lay)
        bn_features = BatchNormalization()(pt_features)
        # here we do an attention mechanism to turn pixels in the GAP on an off
        attn_layer = Conv2D(64, kernel_size = (1,1), padding = 'same', activation = 'relu')(bn_features)
        attn_layer = Conv2D(16, kernel_size = (1,1), padding = 'same', activation = 'relu')(attn_layer)
        attn_layer = LocallyConnected2D(1, kernel_size = (1,1), padding = 'valid', activation = 'sigmoid')(attn_layer)
        # fan it out to all of the channels
        pt_depth = conv_base.get_output_shape_at(0)[-1]
        up_c2_w = np.ones((1, 1, 1, pt_depth))
        up_c2 = Conv2D(pt_depth, kernel_size = (1,1), padding = 'same', 
                    activation = 'linear', use_bias = False, weights = [up_c2_w])
        up_c2.trainable = False
        attn_layer = up_c2(attn_layer)

        mask_features = multiply([attn_layer, bn_features])
        gap_features = GlobalAveragePooling2D()(mask_features)
        gap_mask = GlobalAveragePooling2D()(attn_layer)
        # to account for missing values from the attention model
        gap = Lambda(lambda x: x[0]/x[1], name = 'RescaleGAP')([gap_features, gap_mask])
        gap_dr = Dropout(0.5)(gap)

        """
        Fully connected layer
        """
        fcn_in = Dense(1024, activation = 'elu')(gap_dr)
        dr_steps = Dropout(0.25)(fcn_in)
        out_layer = Dense(1, activation = 'sigmoid')(dr_steps)

        model = Model(inputs = [in_lay], outputs = [out_layer])
        model.summary()

        model.compile(
            optimizer='adam',
            loss='binary_crossentropy',
            metrics=[
                tf.keras.metrics.BinaryAccuracy(name='accuracy'),
                tf.keras.metrics.Precision(name='precision'),
                tf.keras.metrics.Recall(name='recall'),
            ]
        )

        model.fit(
            x=self.x_train,
            y=self.y_train,
            batch_size=32,
            epochs=50,
            callbacks=[
                tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)
            ],
            verbose='auto'
        )

        return model


    def category_case_basic_flow(self):
        input_shape = (256, 256, 3)
        in_lay = Input(input_shape)
        # conv_base = Xception(
        #     include_top = False,
        #     weights = 'imagenet',
        #     input_shape = input_shape
        # )
        conv_base = self.choose_convolutional_model()

        """
        Dynamic Spatial Attention
        """
        pt_features = conv_base(in_lay)
        bn_features = BatchNormalization()(pt_features)
        # here we do an attention mechanism to turn pixels in the GAP on an off
        attn_layer = Conv2D(64, kernel_size = (1,1), padding = 'same', activation = 'relu')(bn_features)
        attn_layer = Conv2D(16, kernel_size = (1,1), padding = 'same', activation = 'relu')(attn_layer)
        attn_layer = LocallyConnected2D(1, kernel_size = (1,1), padding = 'valid', activation = 'sigmoid')(attn_layer)
        # fan it out to all of the channels
        pt_depth = conv_base.get_output_shape_at(0)[-1]
        up_c2_w = np.ones((1, 1, 1, pt_depth))
        up_c2 = Conv2D(pt_depth, kernel_size = (1,1), padding = 'same', 
                    activation = 'linear', use_bias = False, weights = [up_c2_w])
        up_c2.trainable = False
        attn_layer = up_c2(attn_layer)

        mask_features = multiply([attn_layer, bn_features])
        gap_features = GlobalAveragePooling2D()(mask_features)
        gap_mask = GlobalAveragePooling2D()(attn_layer)
        # to account for missing values from the attention model
        gap = Lambda(lambda x: x[0]/x[1], name = 'RescaleGAP')([gap_features, gap_mask])
        gap_dr = Dropout(0.5)(gap)

        """
        Fully connected layer
        """
        fcn_in = Dense(1024, activation = 'elu')(gap_dr)
        dr_steps = Dropout(0.25)(fcn_in)
        out_layer = Dense(11, activation = 'softmax')(dr_steps)

        model = Model(inputs = [in_lay], outputs = [out_layer])
        model.summary()

        model.compile(
            optimizer='adam',
            loss='sparse_categorical_crossentropy',
            metrics=['accuracy']
        )

        model.fit(
            x=self.x_train,
            y=self.y_train,
            batch_size=32,
            epochs=50,
            callbacks=[
                tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)
            ],
            verbose='auto'
        )
        y_pred = model.predict(self.x_test).argmax(axis=1)

        return y_pred


    # def transfer_flow(self):
    #     base_model = self.choose_convolutional_model()
    #
    #     bf_train = base_model.predict(self.x_train)
    #     bf_train = bf_train.reshape(bf_train.shape[0], -1)
    #     bf_test = base_model.predict(self.x_test)
    #     bf_test = bf_test.reshape(bf_test.shape[0], -1)
    #     print("\nBottleneck Features:\nbf_train: {}\nbf_test: {}\n".format(bf_train.shape, bf_test.shape))
    #
    #     clf = self.choose_ml_classifier()
    #
    #     clf.fit(bf_train, self.y_train)
    #     y_pred = clf.predict(bf_test)
    #
    #     del bf_train, bf_test
    #
    #     return y_pred

