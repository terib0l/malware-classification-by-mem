import tensorflow as tf
from tensorflow.keras.models import Model
from model.basic import BasicModel
from module.schema import CaseLabel

# link: https://medium.com/dive-into-ml-ai/experimenting-with-convolutional-block-attention-module-cbam-6325a4e2a70f


def channel_attention(features: int, reduction: int = 16, name: str = "") -> tf.keras.models.Model:
    """channel attention model
    Args:
        features (int): number of features for incoming tensor
        reduction (int, optional): Reduction ratio for the MLP to squeeze information across channels. Defaults to 16.
        name (str, optional): Defaults to "".
    Returns:
        tf.keras.models.Model: channelwise attention appllier model
    """

    input_tensor = tf.keras.layers.Input(shape=(None, None, features))

    # Average pool over a feature map across channels
    avg = tf.reduce_mean(input_tensor, axis=[1, 2], keepdims=True)
    # Max pool over a feature map across channels
    max_pool = tf.reduce_max(input_tensor, axis=[1, 2], keepdims=True)

    # Number of features for middle layer of shared MLP
    reduced_features = int(features // reduction)

    dense1 = tf.keras.layers.Dense(reduced_features)
    avg_reduced = dense1(avg)
    max_reduced = dense1(max_pool)

    dense2 = tf.keras.layers.Dense(features)
    avg_attention = dense2(tf.keras.layers.Activation("relu")(avg_reduced))
    max_attention = dense2(tf.keras.layers.Activation("relu")(max_reduced))

    # Channel-wise attention
    overall_attention = tf.keras.layers.Activation("sigmoid")(avg_attention + max_attention)

    return tf.keras.models.Model(
        inputs=input_tensor, outputs=input_tensor * overall_attention, name=name
    )


def spatial_attention(
    features: int, kernel: int = 7, bias: bool = False, name: str = ""
) -> tf.keras.models.Model:
    """spatial attention model
    Args:
        features (int): number of features for incoming tensor
        kernel (int): convolutional kernel size
        bias (bool, optional): whether to use bias in convolutional layer
        name (str, optional): Defaults to "".
    Returns:
        tf.keras.models.Model: spatial attention appllier model
    """

    input_tensor = tf.keras.layers.Input(shape=(None, None, features))
    # Average pool across channels for a given spatial location
    avg = tf.reduce_mean(input_tensor, axis=[-1], keepdims=True)

    # Max pool across channels for a given spatial location
    max_pool = tf.reduce_max(input_tensor, axis=[-1], keepdims=True)

    concat_pool = tf.concat([avg, max_pool], axis=-1)

    # Attention for spatial locations
    conv = tf.keras.layers.Conv2D(
        1, (kernel, kernel), strides=(1, 1), padding="same", use_bias=bias
    )(concat_pool)
    attention = tf.keras.layers.Activation("sigmoid")(tf.keras.layers.BatchNormalization()(conv))

    return tf.keras.models.Model(inputs=input_tensor, outputs=input_tensor * attention, name=name)


class CbamAttention(BasicModel):
    def binary_case_basic_flow(self):
        conv_base = self.choose_convolutional_model()

        input = tf.keras.layers.Input(shape=(256, 256, 3))
        x = conv_base(input)
        x = channel_attention(features=x.shape[-1], name="channel_attention")(x)
        x = spatial_attention(features=x.shape[-1], name="spatial_attention")(x)
        x = tf.keras.layers.Flatten()(x)
        x = tf.keras.layers.Dense(64, activation='relu')(x)
        x = tf.keras.layers.Dense(1, activation='sigmoid')(x)
        model = tf.keras.models.Model(input, x)
        model.summary()

        model.compile(
            optimizer='adam',
            loss='binary_crossentropy',
            metrics=[
                tf.keras.metrics.BinaryAccuracy(name='accuracy'),
                tf.keras.metrics.Precision(name='precision'),
                tf.keras.metrics.Recall(name='recall'),
            ]
        )

        model.fit(
            x=self.x_train,
            y=self.y_train,
            batch_size=32,
            epochs=50,
            callbacks=[
                tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)
            ],
            verbose='auto'
        )

        return model


    def category_case_basic_flow(self):
        conv_base = self.choose_convolutional_model()

        input = tf.keras.layers.Input(shape=(256, 256, 3))
        x = conv_base(input)
        x = channel_attention(features=x.shape[-1], name="channel_attention")(x)
        x = spatial_attention(features=x.shape[-1], name="spatial_attention")(x)
        x = tf.keras.layers.Flatten()(x)
        x = tf.keras.layers.Dense(64, activation='relu')(x)
        x = tf.keras.layers.Dense(11, activation='softmax')(x)
        model = tf.keras.models.Model(input, x)
        model.summary()

        model.compile(
            optimizer='adam',
            loss='sparse_categorical_crossentropy',
            metrics=['accuracy']
        )

        model.fit(
            x=self.x_train,
            y=self.y_train,
            batch_size=32,
            epochs=50,
            callbacks=[
                tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)
            ],
            verbose='auto'
        )
        y_pred = model.predict(self.x_test).argmax(axis=1)

        return y_pred


    def transfer_flow(self):
        conv_base = self.choose_convolutional_model()

        input = tf.keras.layers.Input(shape=(256, 256, 3))
        x = conv_base(input)
        x = channel_attention(features=x.shape[-1], name="channel_attention")(x)
        x = spatial_attention(features=x.shape[-1], name="spatial_attention")(x)
        x = tf.keras.layers.Flatten()(x)
        x = tf.keras.layers.Dense(64, activation='relu')(x)
        if self.problem_case == CaseLabel.BINARY:
            x = tf.keras.layers.Dense(1, activation='sigmoid')(x)
            model = tf.keras.models.Model(input, x)
            model.summary()
            model.compile(
                optimizer='adam',
                loss='binary_crossentropy',
                metrics=[
                    tf.keras.metrics.BinaryAccuracy(name='accuracy'),
                    tf.keras.metrics.Precision(name='precision'),
                    tf.keras.metrics.Recall(name='recall'),
                ]
            )
            model.fit(
                x=self.x_train,
                y=self.y_train,
                batch_size=32,
                epochs=50,
                callbacks=[
                    tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)
                ],
                verbose='auto'
            )
        elif self.problem_case == CaseLabel.CATEGORY:
            x = tf.keras.layers.Dense(11, activation='softmax')(x)
            model = tf.keras.models.Model(input, x)
            model.summary()
            model.compile(
                optimizer='adam',
                loss='sparse_categorical_crossentropy',
                metrics=['accuracy']
            )
            model.fit(
                x=self.x_train,
                y=self.y_train,
                batch_size=32,
                epochs=50,
                callbacks=[
                    tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)
                ],
                verbose='auto'
            )
        else:
            raise NotImplementedError

        base_model = Model(model.input, model.layers[-3].output)
        base_model.summary()
    
        bf_train = base_model.predict(self.x_train)
        bf_train = bf_train.reshape(bf_train.shape[0], -1)
        bf_test = base_model.predict(self.x_test)
        bf_test = bf_test.reshape(bf_test.shape[0], -1)
        print("\nBottleneck Features:\nbf_train: {}\nbf_test: {}\n".format(bf_train.shape, bf_test.shape))
    
        clf = self.choose_ml_classifier()
    
        clf.fit(bf_train, self.y_train)
        y_pred = clf.predict(bf_test)
    
        del bf_train, bf_test
    
        return y_pred

