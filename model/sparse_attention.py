import numpy as np
import tensorflow as tf
from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout, Input, Conv2D, multiply, LocallyConnected2D, Lambda, BatchNormalization
from tensorflow.keras.models import Model
from model.base import BaseModel
from module.schema import CaseLabel


# link: https://stackoverflow.com/questions/65598647/dynamic-spatial-convolution-is-not-supported-on-tpu
def dynamic_spatial_module(conv_base, problem_case: str, input_shape: tuple):
    """
    Dynamic Spatial Attention
    """
    in_lay = Input(input_shape)

    pt_features = conv_base(in_lay)
    bn_features = BatchNormalization()(pt_features)
    # here we do an attention mechanism to turn pixels in the GAP on an off
    attn_layer = Conv2D(64, kernel_size = (1,1), padding = 'same', activation = 'relu')(bn_features)
    attn_layer = Conv2D(16, kernel_size = (1,1), padding = 'same', activation = 'relu')(attn_layer)
    attn_layer = LocallyConnected2D(1, kernel_size = (1,1), padding = 'valid', activation = 'sigmoid')(attn_layer)
    # fan it out to all of the channels
    pt_depth = conv_base.get_output_shape_at(0)[-1]
    up_c2_w = np.ones((1, 1, 1, pt_depth))
    up_c2 = Conv2D(pt_depth, kernel_size = (1,1), padding = 'same', 
                activation = 'linear', use_bias = False, weights = [up_c2_w])
    up_c2.trainable = False
    attn_layer = up_c2(attn_layer)

    mask_features = multiply([attn_layer, bn_features])
    gap_features = GlobalAveragePooling2D()(mask_features)
    gap_mask = GlobalAveragePooling2D()(attn_layer)
    # to account for missing values from the attention model
    gap = Lambda(lambda x: x[0]/x[1], name = 'RescaleGAP')([gap_features, gap_mask])
    gap_dr = Dropout(0.5)(gap)

    """
    Fully connected layer
    """
    fcn_in = Dense(1024, activation = 'elu')(gap_dr)
    # fcn_in = Dense(64, activation = 'elu')(gap_dr)  # 32, 64, 128, 256, 512, 1024
    dr_steps = Dropout(0.25)(fcn_in)
    if problem_case == CaseLabel.BINARY:
        out_layer = Dense(2, activation = 'softmax')(dr_steps)
    elif problem_case == CaseLabel.CATEGORY:
        out_layer = Dense(11, activation = 'softmax')(dr_steps)
    else:
        raise NotImplementedError

    model = Model(inputs = [in_lay], outputs = [out_layer])
    return model


class DynamicSpatialAttention(BaseModel):
    def apply_cost_sensitive(self):
        """class_weight
        negative : ラベル0の数
        positive : ラベル1の数
        weight_for_0 : 1. / negative * (negative + positive)
        weight_for_1 : 1. / positive * (negative + positive)
        class_weight = {0 : weight_for_0, 1 : weight_for_1}
        """
        if self.problem_case == CaseLabel.BINARY:
            positive = np.count_nonzero(self.y_train)
            negative = len(self.y_train) - positive
            weight_for_0 = 1. / negative * (negative + positive)
            weight_for_1 = 1. / positive * (negative + positive)
            class_weight = {0: weight_for_0, 1: weight_for_1}
            print(
                "Cost-Sensitive:",
                f"\tAll         : {len(self.y_train)}",
                f"\tPositive    : {positive}",
                f"\tNegative    : {negative}",
                f"\t----------------------------",
                f"\tClass weight: {class_weight}",
                sep="\n"
            )
        elif self.problem_case == CaseLabel.CATEGORY:
            a = np.count_nonzero(self.y_train==0)
            b = np.count_nonzero(self.y_train==1)
            c = np.count_nonzero(self.y_train==2)
            d = np.count_nonzero(self.y_train==3)
            e = np.count_nonzero(self.y_train==4)
            f = np.count_nonzero(self.y_train==5)
            g = np.count_nonzero(self.y_train==6)
            h = np.count_nonzero(self.y_train==7)
            i = np.count_nonzero(self.y_train==8)
            j = np.count_nonzero(self.y_train==9)
            k = np.count_nonzero(self.y_train==10)
            abcdefghijk = len(self.y_train)
            weight_for_0 = 1. / a * abcdefghijk
            weight_for_1 = 1. / b * abcdefghijk
            weight_for_2 = 1. / c * abcdefghijk
            weight_for_3 = 1. / d * abcdefghijk
            weight_for_4 = 1. / e * abcdefghijk
            weight_for_5 = 1. / f * abcdefghijk
            weight_for_6 = 1. / g * abcdefghijk
            weight_for_7 = 1. / h * abcdefghijk
            weight_for_8 = 1. / i * abcdefghijk
            weight_for_9 = 1. / j * abcdefghijk
            weight_for_10 = 1. / k * abcdefghijk
            class_weight = {
                0: weight_for_0,
                1: weight_for_1,
                2: weight_for_2,
                3: weight_for_3,
                4: weight_for_4,
                5: weight_for_5,
                6: weight_for_6,
                7: weight_for_7,
                8: weight_for_8,
                9: weight_for_9,
                10: weight_for_10,
            }
            print(
                "Cost-Sensitive:",
                f"\tAll: {abcdefghijk}",
                f"\t0  : {a}",
                f"\t1  : {b}",
                f"\t2  : {c}",
                f"\t3  : {d}",
                f"\t4  : {e}",
                f"\t5  : {f}",
                f"\t6  : {g}",
                f"\t7  : {h}",
                f"\t8  : {i}",
                f"\t9  : {j}",
                f"\t10 : {k}",
                f"\t----------------------------",
                f"\tClass weight: {class_weight}",
                sep="\n"
            )
        else:
            raise NotImplementedError

        return class_weight


    def binary_case_basic_flow(self):
        conv_base = self.choose_convolutional_model()
        _, h, w, d = self.x_train.shape

        model = dynamic_spatial_module(conv_base, self.problem_case, (h, w, d))
        model.summary()

        # Addition
        class_weight = {}
        class_weight = self.apply_cost_sensitive()

        model.compile(
            optimizer=tf.keras.optimizers.Adam(
                # learning_rate=1e-5,
            ),
            # optimizer = tf.keras.optimizers.RMSprop(
            #     learning_rate=1e-5,
            # ),
            loss='sparse_categorical_crossentropy',
            metrics=['accuracy']
        )

        model.fit(
            x=self.x_train,
            y=self.y_train,
            batch_size=32,
            epochs=20,
            callbacks=[
                tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)
            ],
            verbose='auto',
            class_weight=class_weight if class_weight else None
        )
        y_pred = model.predict(self.x_test).argmax(axis=1)
        return y_pred


    def category_case_basic_flow(self):
        conv_base = self.choose_convolutional_model()
        _, h, w, d = self.x_train.shape

        model = dynamic_spatial_module(conv_base, self.problem_case, (h, w, d))
        model.summary()

        model.compile(
            optimizer=tf.keras.optimizers.Adam(
                # learning_rate=1e-5,
            ),
            # optimizer = tf.keras.optimizers.RMSprop(
            #     learning_rate=1e-5,
            # ),
            loss='sparse_categorical_crossentropy',
            metrics=['accuracy']
        )

        model.fit(
            x=self.x_train,
            y=self.y_train,
            batch_size=32,
            epochs=20,
            callbacks=[
                tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)
            ],
            verbose='auto'
        )
        y_pred = model.predict(self.x_test).argmax(axis=1)
        return y_pred

