import numpy as np
from sklearn.metrics import confusion_matrix

def output_metrix_from_matrix(y_test, y_pred, labels_num):
    matrix = confusion_matrix(y_test, y_pred)
    TP = []
    FP = []
    TN = []
    FN = []

    for k in range(labels_num):
        fp = 0
        fn = 0
        tn = 0
        for i in range(labels_num):
            for j in range(labels_num):
                if k == i and k == j:
                    TP.append(matrix[i][j])
                elif k == i and k != j:
                    fp += matrix[i][j]
                elif k != i and k == j:
                    fn += matrix[i][j]
                else:
                    tn += matrix[i][j]
        FP.append(fp)
        FN.append(fn)
        TN.append(tn)

    TPs = sum(TP)
    accuracy = TPs / sum(list(map(sum, zip(*matrix))))

    precision = list()
    for i in range(labels_num):
      precision.append(TP[i] / (TP[i] + FP[i]))

    recall = list()
    for i in range(labels_num):
      recall.append(TP[i] / (TP[i] + FN[i]))

    f1_score = list()
    for i in range(labels_num):
      f1_score.append((2 * precision[i] * recall[i]) / (precision[i] + recall[i]))

    fpr = list()
    for i in range(labels_num):
      fpr.append(FP[i] / (FP[i] + TN[i]))

    print('Accuracy: ', accuracy)
    print('Precision: ', np.nansum(precision) / labels_num)
    print('Recall: ', np.nansum(recall) / labels_num)
    print('F1-Score: ', np.nansum(f1_score) / labels_num)
    print('FPR: ', np.nansum(fpr) / labels_num)
